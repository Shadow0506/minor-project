# ü§ñ Multi-Robot Reinforcement Learning with ARGoS

## Complete Implementation of Deep Q-Learning for Swarm Navigation

[![ARGoS](https://img.shields.io/badge/ARGoS-3.0-blue)](https://www.argos-sim.info/)
[![Python](https://img.shields.io/badge/Python-3.8+-green)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-Educational-yellow)]()

---

## üìñ Quick Navigation

| Document | Description |
|----------|-------------|
| **[QUICKSTART.md](QUICKSTART.md)** | üöÄ Fast setup and run instructions |
| **[BUILD_INSTRUCTIONS.md](BUILD_INSTRUCTIONS.md)** | üõ†Ô∏è Detailed build guide and troubleshooting |
| **[ARCHITECTURE.md](ARCHITECTURE.md)** | üß† In-depth technical explanation |
| **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)** | üìã Complete project overview |

---

## üéØ What This Project Does

Four FootBots learn to navigate from starting positions to a goal at (18, 18) in a 20√ó20m arena while:
- ‚úÖ Avoiding walls
- ‚úÖ Avoiding collisions with each other
- ‚úÖ Finding optimal paths
- ‚úÖ Learning purely from rewards (no explicit programming)

**Technology**: ARGoS simulation + Deep Q-Learning (PyTorch)

---

## üé¨ Demo

### What You'll See:

**Episode 1-100**: Random movement, frequent crashes  
**Episode 100-300**: Learning wall avoidance  
**Episode 300-600**: Goal-directed navigation  
**Episode 600-1000**: Coordinated multi-robot paths  

### Training Progress Curve:
![Training Curve Example](https://via.placeholder.com/600x300.png?text=Your+Training+Curve+Will+Appear+Here)

*After training, run `python python/visualize.py plot` to generate your own curve*

---

## ‚ö° Quick Start (5 Minutes)

### Prerequisites
```bash
# Check installations
argos3 --version        # ARGoS 3.x
python --version        # Python 3.8+
cmake --version         # CMake 3.10+
```

### Setup

**Windows:**
```cmd
# 1. Install Python dependencies
cd python
pip install -r requirements.txt

# 2. Build C++ controller
cd ..
build.bat

# 3. Run (in two separate terminals)
run_server.bat     # Terminal 1
run_argos.bat      # Terminal 2
```

**Linux/Mac:**
```bash
# 1. Install Python dependencies
cd python
pip install -r requirements.txt

# 2. Build C++ controller
cd ..
chmod +x build.sh
./build.sh

# 3. Run (in two separate terminals)
python python/q_server.py                      # Terminal 1
argos3 -c experiments/q_swarm_experiment.argos # Terminal 2
```

### Test Before Running
```bash
cd python
python test_system.py  # Verify everything is working
```

---

## üìÅ Project Structure

```
project_files/
‚îÇ
‚îú‚îÄ‚îÄ üìò README.md                    ‚Üê You are here
‚îú‚îÄ‚îÄ üìó QUICKSTART.md                ‚Üê Fast setup guide  
‚îú‚îÄ‚îÄ üìô BUILD_INSTRUCTIONS.md        ‚Üê Detailed build guide
‚îú‚îÄ‚îÄ üìï ARCHITECTURE.md              ‚Üê Technical deep dive
‚îú‚îÄ‚îÄ üìî PROJECT_SUMMARY.md           ‚Üê Complete overview
‚îÇ
‚îú‚îÄ‚îÄ üîß build.bat / build.sh         ‚Üê Build scripts
‚îú‚îÄ‚îÄ ‚ñ∂Ô∏è run_server.bat               ‚Üê Start Python server (Windows)
‚îú‚îÄ‚îÄ ‚ñ∂Ô∏è run_argos.bat                ‚Üê Start ARGoS (Windows)
‚îÇ
‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îî‚îÄ‚îÄ q_swarm_controller/
‚îÇ       ‚îú‚îÄ‚îÄ q_swarm_controller.h       # C++ header
‚îÇ       ‚îú‚îÄ‚îÄ q_swarm_controller.cpp     # C++ implementation
‚îÇ       ‚îî‚îÄ‚îÄ CMakeLists.txt             # Build config
‚îÇ
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îî‚îÄ‚îÄ q_swarm_experiment.argos       # ARGoS config
‚îÇ
‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îú‚îÄ‚îÄ q_network.py                   # Deep Q-Network
‚îÇ   ‚îú‚îÄ‚îÄ q_server.py                    # Communication server
‚îÇ   ‚îú‚îÄ‚îÄ visualize.py                   # Plotting tools
‚îÇ   ‚îú‚îÄ‚îÄ test_system.py                 # Test suite
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt               # Dependencies
‚îÇ
‚îî‚îÄ‚îÄ models/                            # (Created during training)
    ‚îú‚îÄ‚îÄ q_network_latest.pth           # Latest checkpoint
    ‚îî‚îÄ‚îÄ training_data.json             # Training stats
```

---

## üß© System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          ARGoS Simulator (C++)              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ FootBot 0‚îÇ  ‚îÇ FootBot 1‚îÇ  ‚îÇ FootBot 2‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ       ‚îÇ State       ‚îÇ State       ‚îÇ State   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ             ‚îÇ             ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ TCP Socket (Port 5555)
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Python Q-Network Server                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ  Deep Q-Network (PyTorch)                ‚îÇ‚îÇ
‚îÇ  ‚îÇ  Input: 28 ‚Üí Hidden: 128√ó3 ‚Üí Output: 4  ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ  Experience Replay Buffer (10k samples)  ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Data Flow**:
1. Robot reads sensors ‚Üí sends state to Python
2. Q-Network selects action ‚Üí sends back to robot
3. Robot executes action in ARGoS
4. Robot calculates reward ‚Üí sends to Python
5. Q-Network learns from experience
6. Repeat for 1000 episodes

---

## üéì Key Features

### ‚ú® Reinforcement Learning
- **Algorithm**: Deep Q-Learning (DQN)
- **Experience Replay**: 10,000 sample buffer
- **Target Network**: Stabilized training
- **Epsilon-Greedy**: Exploration vs exploitation

### ü§ñ Multi-Robot System
- 4 FootBots learning simultaneously
- Shared Q-Network (centralized learning)
- Individual state-action pairs
- Emergent coordination

### üî¨ Physics Simulation
- ARGoS 2D dynamics engine
- Realistic robot kinematics
- Collision detection
- Real-time visualization

### üíª Software Engineering
- C++/Python interoperability
- Socket-based IPC
- Modular architecture
- Comprehensive documentation

---

## üìä Training Specifications

| Parameter | Value |
|-----------|-------|
| Arena Size | 20 √ó 20 m |
| Robots | 4 FootBots |
| Episodes | 1000 |
| Steps/Episode | 500 max |
| State Dimensions | 28 (position + 24 proximity sensors) |
| Actions | 4 (forward, left, right, stop) |
| Rewards | +10 goal, -5 collision, -0.1 step |
| Learning Rate | 0.001 |
| Discount Factor Œ≥ | 0.99 |
| Epsilon Decay | 0.995 per episode |

---

## üìà Expected Results

### Training Metrics

After 1000 episodes:
- **Success Rate**: >70% reach goal
- **Average Reward**: +6 to +9 (from initial -15)
- **Collision Rate**: <10% (from initial 60%)
- **Path Efficiency**: Near-optimal

### Checkpoints

Models saved every 25 episodes:
- `models/q_network_episode_25.pth`
- `models/q_network_episode_50.pth`
- ...
- `models/q_network_final.pth`

---

## üîß Advanced Usage

### Visualize Training

```bash
cd python

# Plot reward curve
python visualize.py plot

# Show statistics
python visualize.py stats

# Test trained model
python visualize.py test
```

### Modify Hyperparameters

Edit `python/q_network.py`:
```python
agent = QNetworkAgent(
    learning_rate=0.001,    # ‚Üê Adjust
    gamma=0.99,             # ‚Üê Adjust
    epsilon=1.0,            # ‚Üê Adjust
    epsilon_decay=0.995     # ‚Üê Adjust
)
```

### Change Reward Structure

Edit `controllers/q_swarm_controller/q_swarm_controller.cpp`:
```cpp
float QSwarmController::CalculateReward(bool& done) {
    float reward = -0.1f;  // ‚Üê Adjust step penalty
    
    if (ReachedGoal()) {
        reward = 10.0f;     // ‚Üê Adjust goal reward
        done = true;
    }
    else if (DetectCollision()) {
        reward = -5.0f;     // ‚Üê Adjust collision penalty
        done = true;
    }
    
    return reward;
}
```

### Add More Robots

Edit `experiments/q_swarm_experiment.argos`:
```xml
<foot-bot id="fb4">
  <body position="6, 2, 0" orientation="0, 0, 0" />
  <controller config="qsc" />
</foot-bot>
```

---

## üêõ Troubleshooting

| Problem | Solution |
|---------|----------|
| "Cannot find controller library" | Use full path in `.argos` file |
| "Connection refused" | Start Python server before ARGoS |
| "PyTorch not found" | `pip install torch` |
| "ARGoS not found" | Verify with `argos3 --version` |
| Robots not moving | Check controller loaded in ARGoS console |

**Full troubleshooting guide**: See [BUILD_INSTRUCTIONS.md](BUILD_INSTRUCTIONS.md)

---

## üìö Learning Resources

### Understanding the Code
1. Start with [QUICKSTART.md](QUICKSTART.md) to run it
2. Read [ARCHITECTURE.md](ARCHITECTURE.md) for theory
3. Study code comments in source files
4. Modify parameters and observe changes

### Deep Q-Learning
- **Paper**: [Playing Atari with Deep RL](https://arxiv.org/abs/1312.5602)
- **Tutorial**: [PyTorch DQN Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)

### ARGoS
- **Documentation**: [ARGoS Manual](https://www.argos-sim.info/core.php)
- **Examples**: [ARGoS Examples](https://github.com/ilpincy/argos3-examples)

---

## üöÄ Extension Ideas

1. **Different Algorithms**
   - PPO (Proximal Policy Optimization)
   - A3C (Asynchronous Actor-Critic)
   - MADDPG (Multi-Agent DDPG)

2. **Enhanced Environment**
   - Moving obstacles
   - Multiple goals
   - Dynamic arena size

3. **Advanced Features**
   - Robot-to-robot communication
   - Formation control
   - Area coverage tasks

4. **Real-World Deployment**
   - Transfer to physical robots
   - ROS integration
   - Hardware-in-the-loop testing

---

## ‚úÖ Complete Feature Checklist

- [x] ARGoS simulation environment (20√ó20m)
- [x] 4 FootBots with differential drive
- [x] Proximity sensors (24 per robot)
- [x] Position sensing (GPS-like)
- [x] Deep Q-Network (PyTorch)
- [x] Experience replay buffer
- [x] Target network for stability
- [x] Epsilon-greedy exploration
- [x] Socket communication (C++ ‚Üî Python)
- [x] Reward function (goal/collision/step)
- [x] Episode management
- [x] Model checkpointing
- [x] Training visualization
- [x] Comprehensive documentation
- [x] Build scripts (Windows/Linux)
- [x] Test suite
- [x] Example results

---

## üìù License

This project is for **educational purposes**. Feel free to use, modify, and learn from it.

---

## üôè Acknowledgments

- **ARGoS Team** - Excellent multi-robot simulator
- **PyTorch Team** - Deep learning framework
- **DeepMind** - DQN algorithm pioneers

---

## üìß Support

Having issues? Check:
1. [BUILD_INSTRUCTIONS.md](BUILD_INSTRUCTIONS.md) - Detailed troubleshooting
2. `python/test_system.py` - System verification
3. ARGoS documentation
4. PyTorch tutorials

---

## üéØ Success Criteria

**You've succeeded when:**
- ‚úÖ System compiles and runs without errors
- ‚úÖ You see 4 robots moving in ARGoS visualization
- ‚úÖ Python console shows improving episode rewards
- ‚úÖ Robots learn to reach goal by episode 500-1000
- ‚úÖ You understand the reinforcement learning process

---

## üåü Final Notes

This is a **complete, production-ready** reinforcement learning system. Every component is:
- ‚úÖ Fully documented
- ‚úÖ Well-commented
- ‚úÖ Modular and extensible
- ‚úÖ Ready to run and learn

**Ready to start?** ‚Üí See [QUICKSTART.md](QUICKSTART.md)

**Want to understand it?** ‚Üí See [ARCHITECTURE.md](ARCHITECTURE.md)

**Having issues?** ‚Üí See [BUILD_INSTRUCTIONS.md](BUILD_INSTRUCTIONS.md)

---

<div align="center">

### ü§ñ Happy Learning! üß†

**Built with ‚ù§Ô∏è for robotics and AI education**

</div>
